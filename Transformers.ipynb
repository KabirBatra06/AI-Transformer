{"cells":[{"cell_type":"markdown","metadata":{"id":"4PEQdCFKiCpy"},"source":["# Transformers\n"]},{"cell_type":"markdown","metadata":{"id":"37lQl27CiCp2"},"source":["### Objective: Build different transformer components and test them."]},{"cell_type":"code","execution_count":49,"metadata":{"executionInfo":{"elapsed":121,"status":"ok","timestamp":1682122979554,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"RVfuhMvZiCp3"},"outputs":[],"source":["import numpy as np\n","import string\n","import time\n","import torch\n","import pdb\n","import math\n","import torch.nn as nn\n","from torch.autograd import Variable\n","from torch.nn import functional as F\n","np.random.seed(124)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"9i88KqXbiCp4"},"source":["### I am using the convention of having the batch dimension first so tensors will have shapes of `(N, L, D)` where `N` is the batch dimension, `L` is the max sequence length, and `D` is the feature dimension.\n","The default in PyTorch is for the sequence dimension to be first, i.e., `(L, N, D)` but most functions in PyTorch can be altered to make the batch dimension to be first by using `batch_first=True`.\n"]},{"cell_type":"markdown","metadata":{"id":"v4h7fkjTiCp4"},"source":["# Positional Encoder\n","\n","The positional encoder is a simple function that takes a 3D tensor of shape (batch_size, sequence_length, encoding_size), i.e., `(N, L, D)`, and returns a 3D tensor of the same shape where positional encoding embedding has been added. The positional encoder is a function of the position of the token in the sequence. The positional encoder is defined as:\n","\n","$$PE_{(pos,2i)} = \\sin(pos/10000^{2i/d_{model}})$$\n","\n","$$PE_{(pos,2i+1)} = \\cos(pos/10000^{2i/d_{model}})$$\n","\n","where $pos$ is the position and $i$ is the dimension. That is, each dimension of the positional encoding corresponds to a sinusoid. The wavelengths form a geometric progression from $2\\pi$ to $10000 \\cdot 2\\pi$.\n","\n","In practice, the positional encoding is added to the embedding vector. This is done by first creating a tensor of shape (1, sequence_length, d_model) and then adding it to the embedding vector. This ensures that the positional encoding is added to every element in the batch via broadcasting."]},{"cell_type":"code","execution_count":50,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":809,"status":"ok","timestamp":1682122980529,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"mss8GHxoiCp4","outputId":"1980912e-2d1b-4620-fb23-56bc8121d82e"},"outputs":[{"name":"stdout","output_type":"stream","text":["torch.Size([1, 4, 512])\n","False\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([0.2263, 1.4525, 0.6788, 1.9051, 1.1314])\n","input_pe: tensor([0.0100, 0.0200, 0.0300, 0.0400, 0.0500]) \n","output_pe: tensor([1.0677, 1.0222, 1.4808, 1.5285, 1.8931])\n","input_pe: tensor([5.0800, 5.0900, 5.1000, 5.1100, 5.1200]) \n","output_pe: tensor([115.9473, 115.1735, 116.3998, 115.6261, 116.8524])\n"]}],"source":["class PositionalEncoder(nn.Module):\n","    def __init__(self, d_model, max_seq_len = 80):\n","        super().__init__()\n","        self.d_model = d_model\n","        \n","        # create constant 'pe' matrix with values dependant on \n","        # pos and i\n","        pe = torch.zeros(max_seq_len, d_model)\n","\n","        # Loop over the positions and the embedding dimensions\n","        # and calculate the positional encoding for each dimension\n","        # and position\n","\n","        for i in range(max_seq_len):\n","          for j in range(d_model):\n","            if j % 2:\n","              pe[i][j] += np.cos(i / (10000 ** (2*j/d_model)))\n","            else:\n","              pe[i][j] += np.sin(i / (10000 ** (2*j/d_model)))\n","        \n","        pe = pe.unsqueeze(0)\n","\n","        self.register_buffer('pe', pe)\n"," \n","    \n","    def forward(self, x):\n","        # make embeddings relatively larger than pe\n","        x = x * math.sqrt(self.d_model)\n","        # add constant positional encoding to embedding\n","        seq_len = x.size(1)\n","        x = x + Variable(self.pe[:,:seq_len], requires_grad=False)\n","        return x\n","\n","pe = PositionalEncoder(512)\n","input_pe = torch.arange(1, 513)*0.01\n","input_pe = input_pe.repeat(1, 4, 1).float()\n","\n","output_pe = pe(input_pe)\n","print(output_pe.shape)\n","\n","# check the difference between the two embeddings\n","print(torch.equal(input_pe, output_pe)) # They should not be equal after adding positional encoding\n","print(f\"input_pe: {input_pe[0, 0, 0:5]} \\noutput_pe: {output_pe[0, 0, 0:5]}\")\n","print(f\"input_pe: {input_pe[0, 1, 0:5]} \\noutput_pe: {output_pe[0, 1, 0:5]}\")\n","print(f\"input_pe: {input_pe[0, 2, -5:]} \\noutput_pe: {output_pe[0, 2, -5:]}\")"]},{"cell_type":"markdown","metadata":{"id":"-5IFO1rYiCp5"},"source":["# Scaled Dot-Product Attention\n","I will implement a version of attention used in transformers:\n","$$\n","A(Q,K,V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right) V\n","$$\n","where $Q \\in \\mathbb{R}^{L\\times d_k}$, $K \\in \\mathbb{R}^{L \\times d_k}$, and $V \\in \\mathbb{R}^{L \\times d_v}$, where $d_v$ is the dimension of the values. The softmax is across the column dimension. The output of attention should be a matrix $A \\in \\mathbb{R}^{L \\times d_v}$.\n","\n","Additionally, I will implement a **batched version** of this that can be used for multiple sequences at the same time. To do this, I will need to use the following **batched** version(s) of matrix multiplication either [`torch.bmm`](https://pytorch.org/docs/stable/generated/torch.bmm.html) or [`torch.matmul`](https://pytorch.org/docs/stable/generated/torch.matmul.html) for both the $QK^T$ and the product of the attention matrix and $V$. We recommend that you use `torch.bmm` as it is more explicit."]},{"cell_type":"code","execution_count":51,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1682122980529,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"8uj4Rr5fiCp5","outputId":"4129a526-0a63-4e16-a0b0-2c0d0b1d84e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["Is shape of output correct? True\n","Is shape of att_values correct? True\n","Do attention values sum to 1? True\n","Output check (first): \n","tensor([-1.3709, -0.6827,  0.3234,  0.8677, -0.1474, -0.9653, -0.7344,  0.8126,\n","         0.1219,  0.3224,  0.6257, -0.0958, -0.1664, -0.0667, -0.2810,  0.3068,\n","        -0.7030, -0.6719,  0.4364, -1.0071,  0.3534,  0.3160,  0.0326, -0.7315,\n","        -0.5165])\n","Output check (last): \n","tensor([-0.2094,  1.3784,  0.2855, -0.1716,  0.1597, -0.6656,  0.3981, -0.9903,\n","        -0.6043, -0.6398,  0.0563, -1.5367, -0.0225, -0.8317,  0.0572,  0.2014,\n","         0.1324, -0.4563,  0.3832,  0.1051,  0.0653, -0.2076,  0.6225, -0.4946,\n","        -0.2935])\n"]}],"source":["def attention(q, k, v):\n","    '''\n","    Inputs:\n","    q: query vector of shape (batch_size, seq_len, d_k)\n","    k: key vector of shape (batch_size, seq_len, d_k)\n","    v: value vector of shape (batch_size, seq_len, d_v)\n","\n","    Returns:\n","    output: attention weighted sum of the value vectors \n","        of shape (batch_size, seq_len, d_k)\n","    scores: attention weights of shape (batch_size, seq_len, seq_len)\n","    '''        \n","    d_k = k.size(-1)\n","    assert d_k == q.size(-1), 'q and k should have the same dimensionality'\n","    d_v = v.size(-1)\n","\n","    K_trans = k.transpose(1, 2)\n","\n","    val = torch.bmm(q, K_trans) / np.sqrt(d_k)\n","\n","    att_values = torch.softmax(val, 2)\n","    output = torch.bmm(att_values, v)\n","\n","    return output, att_values\n","\n","# test the attention function with some random values\n","torch.manual_seed(42)\n","q = torch.randn(2, 5, 512)\n","k = torch.randn(2, 5, 512)\n","v = torch.randn(2, 5, 256)\n","\n","output, att_values = attention(q, k, v)\n","print(f\"Is shape of output correct? {output.shape == v.shape}\")\n","print(f\"Is shape of att_values correct? {att_values.shape == torch.Size([q.shape[0], q.shape[1], q.shape[1]])}\")\n","print(f\"Do attention values sum to 1? {torch.allclose(torch.sum(att_values, dim=-1), torch.ones(1))}\")\n","\n","# Last 25 values of last sample and last token\n","print(f\"Output check (first): \\n{output[0,0,:25]}\")\n","print(f\"Output check (last): \\n{output[-1,-1,-25:]}\")\n"]},{"cell_type":"markdown","metadata":{"id":"Pg9n-Dx2iCp6"},"source":["# Attention modules\n","\n","## Self-attention module\n","I implement a self-attention module that takes in `x` and computes `q`,`k`,`v` internally using 3 linear layers. Then, I use the function from above to compute the output and attention and return it. The attention module takes as constructor parameters the `input_dim`, `key_dim`, and the `output_dim`."]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682122980529,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"Tjo5_awHiCp6","outputId":"ee76a719-8afb-4c12-ca5b-751d838f1b69"},"outputs":[{"name":"stdout","output_type":"stream","text":["input shape: torch.Size([4, 10, 512])\n","output shape: torch.Size([4, 10, 512])\n","Input: \n","tensor([-0.1988, -0.3060,  0.6383,  0.5713,  1.2769])\n","Output: \n","tensor([ 8.6836e-02, -4.1725e-02, -2.5798e-01, -9.6712e-02,  3.0022e-05],\n","       grad_fn=<SliceBackward0>)\n","\n","Does the module exhibit permutation-equivaraince? True\n","The following two lines should be the same:\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward0>)\n","tensor([ 0.1849,  0.3706,  0.1488,  0.0898,  0.2528,  0.4277,  0.4572,  0.1172,\n","         0.0893, -0.2081], grad_fn=<SliceBackward0>)\n"]}],"source":["class SelfAttention(nn.Module):\n","    def __init__(self, input_dim, key_dim, output_dim):\n","        super().__init__()\n","\n","        self.lin_q = nn.Linear(input_dim, key_dim)\n","        self.lin_k = nn.Linear(input_dim, key_dim)\n","        self.lin_v = nn.Linear(input_dim, output_dim)\n","\n","\n","    def forward(self, x):\n","        '''\n","        `x` has shape (batch_dim, sequence_length, input_dim) or (N, L, D_in)\n","        \n","        The output should have shape (batch_dim, sequence_length, output_dim) or (N, L, D_out).\n","        '''\n","\n","        k = self.lin_k(x)\n","        q = self.lin_q(x)\n","        v = self.lin_v(x)\n","        \n","        output, val = attention(q, k, v)\n","\n","\n","        return output\n","    \n","# test the self-attention module with some random values\n","torch.manual_seed(48)\n","input_dim = 512\n","key_dim = 64\n","output_dim = 512\n","self_attn = SelfAttention(input_dim, key_dim, output_dim)\n","x = torch.randn(4, 10, 512)\n","output = self_attn(x)\n","print(f\"input shape: {x.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'Input: \\n{x[0,0,:5]}\\nOutput: \\n{output[0,0,:5]}\\n')\n","\n","# For self-attention, lets check the permutation-equivariant property, \n","# i.e., permute the input sequence and check if the output sequence is also permuted but otherwise the same. \n","# This is a nice sanity check that self-attention is working properly.\n","random_permutation = torch.randperm(x.size(1))\n","reverse_permutation = torch.zeros_like(random_permutation)\n","reverse_permutation[random_permutation] = torch.arange(len(random_permutation))\n","assert torch.all(x[:, random_permutation, :][:, reverse_permutation, :] == x), 'inverse is incorrect'\n","\n","x_prime = x[:, random_permutation, :] # Permute input\n","output_prime = self_attn(x_prime)\n","output_prime_permuted = output_prime[:, reverse_permutation, :]  # Reverse permutation of output\n","print(f'Does the module exhibit permutation-equivaraince?'\n","      f' {torch.allclose(output, output_prime_permuted, atol=1e-5, rtol=1)}')\n","print(f'The following two lines should be the same:')\n","print(output[-1,-1,:10])\n","print(output_prime_permuted[-1,-1,:10])"]},{"cell_type":"markdown","metadata":{"id":"QdlQ1dvuiCp7"},"source":["# Cross Attention module\n","For cross attention, there will be an first input `x` that will correspond to the query and a second input `y` that will correspond to the keys and values. (In self-attention, `x` and `y` were equal).\n","This should be the same basic idea except that there is a linear layer to compute `q` from `x` and linear layers to compute `k` and `v` from `y`."]},{"cell_type":"code","execution_count":53,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":142,"status":"ok","timestamp":1682122980669,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"e6ZXK6a6iCp8","outputId":"949bb773-0239-4c15-9c03-0e9e65c59435"},"outputs":[{"name":"stdout","output_type":"stream","text":["input shape x and y: torch.Size([3, 10, 512]), torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 128])\n","x\n","tensor([-0.0385,  0.9773, -1.4370,  0.8719, -2.1034, -0.2877,  0.3034, -1.9151,\n","         1.1799,  0.6151])\n","y\n","tensor([-2.1565,  0.2397,  0.5872,  0.3950, -0.6114,  0.3489, -0.3467,  0.2792,\n","        -1.2541,  0.4053])\n","output\n","tensor([-0.1544,  0.0847,  0.2329,  0.0549, -0.1424,  0.0711,  0.0105, -0.2139,\n","        -0.0208, -0.1942], grad_fn=<SliceBackward0>)\n"]}],"source":["class CrossAttention(nn.Module):\n","    def __init__(self, x_input_dim, y_input_dim, key_dim, output_dim):\n","        super().__init__()\n","        \n","        self.lin_q = nn.Linear(x_input_dim, key_dim)\n","        self.lin_k = nn.Linear(y_input_dim, key_dim)\n","        self.lin_v = nn.Linear(y_input_dim, output_dim)\n","\n","    def forward(self, x, y):\n","\n","        k = self.lin_k(y)\n","        q = self.lin_q(x)\n","        v = self.lin_v(y)\n","        output, val = attention(q, k, v)\n","\n","        \n","        return output\n","\n","# test the attention module with some random values\n","torch.manual_seed(14)\n","x_input_dim = 512\n","y_input_dim = 256\n","key_dim = 64\n","output_dim = 128\n","cross_attn = CrossAttention(x_input_dim, y_input_dim, key_dim, output_dim)\n","x = torch.randn(3, 10, x_input_dim)\n","y = torch.randn(3, 10, y_input_dim)\n","output = cross_attn(x, y)\n","print(f\"input shape x and y: {x.shape}, {y.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'x\\n{x[0,0,:10]}')\n","print(f'y\\n{y[0,0,:10]}')\n","print(f'output\\n{output[0,0,:10]}')"]},{"cell_type":"markdown","metadata":{"id":"SVASIjOHiCp8"},"source":["## Multi-headed self-attention module\n","Multi-headed self-attention merely passes the the input to each attention module, concatenates all the outputs, and then applies a linear layer to get the final output.\n","Implementation of multi-headed attention below."]},{"cell_type":"code","execution_count":54,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1682122980669,"user":{"displayName":"Kabir Batra","userId":"16958623843803391509"},"user_tz":240},"id":"_QTaI5Z6iCp9","outputId":"94c4b5e9-dda0-4fb8-c44e-8146b0dc56d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["input shape: torch.Size([3, 10, 256])\n","output shape: torch.Size([3, 10, 32])\n","x\n","tensor([ 0.7195, -0.3636,  1.3771,  0.3482, -0.0604, -0.3034, -0.0698,  0.2131,\n","        -0.9736, -0.4651])\n","output\n","tensor([ 0.2290, -0.0350,  0.0918, -0.1069, -0.1679, -0.1939, -0.2167,  0.1841,\n","         0.0546,  0.1668], grad_fn=<SliceBackward0>)\n"]}],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self, attn_modules, final_output_dim):\n","        super().__init__()\n","\n","        self.attn_mod = nn.ModuleList(attn_modules)\n","\n","        out_dim = sum([a.lin_v.out_features for a in attn_modules])\n","\n","        self.lin = nn.Linear(out_dim, final_output_dim)\n","\n","\n","    def forward(self, x):\n","\n","        outputs = [a(x) for a in self.attn_mod]\n","\n","        conc_out = torch.cat(outputs, -1)\n","\n","        output = self.lin(conc_out)\n","\n","\n","        return output\n","    \n","# test the multi-headed attention module with some random values\n","torch.manual_seed(10)\n","input_dim = 256\n","key_dim = 128 \n","output_dim = 64 \n","final_output_dim = 32\n","num_heads = 8 \n","attn_modules = [SelfAttention(input_dim, key_dim, output_dim//num_heads) for _ in range(num_heads)]\n","multi_attn = MultiHeadedAttention(attn_modules, final_output_dim)\n","\n","x = torch.randn(3, 10, input_dim)\n","output = multi_attn(x)\n","print(f\"input shape: {x.shape}\")\n","print(f\"output shape: {output.shape}\")\n","print(f'x\\n{x[0,0,:10]}\\noutput\\n{output[0,0,:10]}')\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"vscode":{"interpreter":{"hash":"b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"}}},"nbformat":4,"nbformat_minor":0}
